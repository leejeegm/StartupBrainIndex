# SBI MVP 성능 한계 테스트 보고

## 1. 테스트 환경 요약

- **서버**: FastAPI + uvicorn **단일 워커** (기본값, `python main.py` 또는 `run_server.bat`)
- **부하 도구**: `load_test.py` (requests + ThreadPoolExecutor)
- **측정 항목**: 동시 사용자(스레드) 수, 총 요청 수, 성공/실패, RPS, 지연(평균·p95·최대)

---

## 2. 테스트 결과 요약

### 2-1. 경량 시나리오 (GET `/` + GET `/health`, 세션 없음)

| 항목 | 결과 (workers=50, 약 5초 유지) |
|------|--------------------------------|
| 총 요청 | 100 (50명 × 2요청) |
| 성공 | 50 (GET `/` 만 200) |
| 실패 | 50 (GET `/health` — 환경에 따라 404 등) |
| RPS | 약 19.6 |
| 지연(전체) | min 42ms, avg 1610ms, **p95 4234ms**, max 4636ms |
| GET `/` 단일 | 50회 성공, avg 2642ms, p95 4546ms |

**해석**: 동시 50명 수준에서도 **GET `/`** 은 전부 성공하나, 요청당 평균·p95 지연이 크게 증가(2~4초대). 단일 워커가 동시 요청을 순차 처리하면서 지연이 쌓인 것으로 보임.

### 2-2. 전체 사용자 플로우 (루트 → 로그인 → 대시보드 → API)

| 항목 | 결과 (workers=10, 약 12초) |
|------|-----------------------------|
| 총 요청 | 60 |
| 성공 | 30 (/, /login, /login POST) |
| 실패 | 30 (/dashboard, /api/me, /health — 세션/리다이렉트 이슈 가능) |
| RPS | 약 10.7 |
| 지연(전체) | min 204ms, avg 891ms, **p95 2137ms**, max 2166ms |

**해석**: 로그인·세션 기반 구간에서 실패가 발생. 동시 10명만 되어도 p95가 2초를 넘고, **실제 동시 접속 허용치는 이보다 낮을 가능성**이 큼.

---

## 3. MVP 현재 구조상 추정 한계

| 구간 | 추정 한계 (현재 단일 워커 기준) | 비고 |
|------|--------------------------------|------|
| **동시 접속(동시 요청 처리)** | **약 10~30명** | 50명부터 p95 4초대, 사용 체감상 한계 |
| **초당 요청 수(RPS)** | **약 10~20** | 부하 시 20 근처에서 지연 급증 |
| **1만 명 동시 접속** | **불가** | 단일 프로세스·단일 워커로는 불가능 |

**병목 요인**  
- uvicorn **워커 1개**: 모든 요청이 한 스레드에서 처리됨.  
- **동기 I/O**: DB(MySQL/SQLite), 설문 채점, PDF 생성, 파일 읽기 등이 블로킹.  
- **무거운 API**: `/api/analyze-sbi`, `/api/generate-pdf` 등은 처리 시간이 길어 동시에 많으면 지연·타임아웃 증가.

---

## 4. 1만 명·실전 테스트를 위한 권장 방향

### 4-1. 단기 (현재 MVP 유지 + 인터넷 공개)

- **인터넷 공개**: 공유기 포트포워딩 또는 **터널(ngrok 등)** 으로 임시 공개 후, 소수 인원으로 회원가입·실제 사용 플로우 검증.
- **동시 접속 기대**: 수십 명 수준까지. 1만 명 동시는 구조 변경 없이는 불가.

### 4-2. 중기 (성능 상향 후 부하 재측정)

1. **멀티 워커**:  
   `uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4`  
   → CPU 코어 수에 맞춰 4~8 워커로 RPS·동시 접속 수 상승 가능.
2. **멀티 워커 실행 예시** (즉시 적용 가능):  
   `uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4`  
   → CPU 코어당 1~2 워커로 RPS·동시 처리량 상승. 재측정 권장.
3. **비동기/경량화**:  
   DB 접근을 async 풀 사용, PDF·무거운 연산은 백그라운드 큐로 분리.
4. **캐시**:  
   설문 문항 목록, 정적 자원 등은 메모리/Redis 캐시로 반복 연산 감소.
5. **부하 테스트 재실행**:  
   `python load_test.py --workers 100 --duration 30 --light`  
   워커 증가 후 동일 스크립트로 다시 측정해 한계치 업데이트.

### 4-3. 1만 명 동시 접속 목표 시

- **클라우드 배포** (AWS, GCP, Azure, 또는 Render/Railway/Fly.io 등).
- **로드밸런서 + 여러 앱 인스턴스** (수십~수백 워커 분산).
- **DB**: MySQL/PostgreSQL 전용 서버 또는 관리형 DB.
- **세션**: 스티키 세션 또는 외부 세션 스토어(Redis 등).

---

## 5. 부하 테스트 실행 방법 (재측정 시)

```bash
# 서버 실행 (별도 터미널)
python main.py
# 또는 run_server.bat

# 경량만 (/, /health) — RPS·동시접속 한계 확인
python load_test.py --base http://127.0.0.1:8000 --workers 50 --duration 20 --light

# 전체 플로우 (로그인~대시보드)
python load_test.py --base http://127.0.0.1:8000 --workers 20 --duration 15
```

결과는 `load_test_report.txt` 에 저장됨.

---

## 6. 결론

| 질문 | 답변 |
|------|------|
| **현재 MVP 최대 사용 퍼포먼스 한계** | **동시 접속 약 10~30명, RPS 약 10~20** 수준. 그 이상은 지연·실패율 급증. |
| **1만 명 동시 접속** | 단일 PC·단일 워커로는 **불가**. 멀티 워커·분산 배포·DB/캐시 구조 변경 필요. |
| **실전 테스트(회원가입·사용)** | 인터넷 공개(포워딩/ngrok/클라우드)로 **소규모(수십 명)** 실사용 테스트는 가능. |

이 보고서는 현재 코드·단일 워커 기준 측정 결과를 바탕으로 한 추정입니다. 워커 수·배포 환경 변경 후 `load_test.py` 로 재측정하면 더 정확한 한계치를 얻을 수 있습니다.
